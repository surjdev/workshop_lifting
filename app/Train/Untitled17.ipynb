{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "xduDmGWiWWjz",
        "outputId": "8697cbf1-cd11-407d-c57d-527c7487dfa2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>keypoint_0_x</th>\n",
              "      <th>keypoint_0_y</th>\n",
              "      <th>keypoint_0_z</th>\n",
              "      <th>keypoint_0_visibility</th>\n",
              "      <th>keypoint_1_x</th>\n",
              "      <th>keypoint_1_y</th>\n",
              "      <th>keypoint_1_z</th>\n",
              "      <th>keypoint_1_visibility</th>\n",
              "      <th>keypoint_2_x</th>\n",
              "      <th>...</th>\n",
              "      <th>keypoint_30_visibility</th>\n",
              "      <th>keypoint_31_x</th>\n",
              "      <th>keypoint_31_y</th>\n",
              "      <th>keypoint_31_z</th>\n",
              "      <th>keypoint_31_visibility</th>\n",
              "      <th>keypoint_32_x</th>\n",
              "      <th>keypoint_32_y</th>\n",
              "      <th>keypoint_32_z</th>\n",
              "      <th>keypoint_32_visibility</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>2.100000</td>\n",
              "      <td>0.524635</td>\n",
              "      <td>0.171705</td>\n",
              "      <td>-0.151355</td>\n",
              "      <td>0.998253</td>\n",
              "      <td>0.523957</td>\n",
              "      <td>0.151215</td>\n",
              "      <td>-0.140882</td>\n",
              "      <td>0.996524</td>\n",
              "      <td>0.523311</td>\n",
              "      <td>...</td>\n",
              "      <td>0.924650</td>\n",
              "      <td>0.515694</td>\n",
              "      <td>0.698783</td>\n",
              "      <td>0.477038</td>\n",
              "      <td>0.924965</td>\n",
              "      <td>0.523433</td>\n",
              "      <td>0.786935</td>\n",
              "      <td>0.077680</td>\n",
              "      <td>0.978275</td>\n",
              "      <td>lift</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>2.133333</td>\n",
              "      <td>0.525219</td>\n",
              "      <td>0.172805</td>\n",
              "      <td>-0.147992</td>\n",
              "      <td>0.998362</td>\n",
              "      <td>0.524634</td>\n",
              "      <td>0.151772</td>\n",
              "      <td>-0.138045</td>\n",
              "      <td>0.996766</td>\n",
              "      <td>0.524023</td>\n",
              "      <td>...</td>\n",
              "      <td>0.929728</td>\n",
              "      <td>0.516461</td>\n",
              "      <td>0.694523</td>\n",
              "      <td>0.503850</td>\n",
              "      <td>0.921965</td>\n",
              "      <td>0.522602</td>\n",
              "      <td>0.784162</td>\n",
              "      <td>0.121329</td>\n",
              "      <td>0.979272</td>\n",
              "      <td>lift</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>2.166667</td>\n",
              "      <td>0.526411</td>\n",
              "      <td>0.173942</td>\n",
              "      <td>-0.152887</td>\n",
              "      <td>0.998442</td>\n",
              "      <td>0.525811</td>\n",
              "      <td>0.153505</td>\n",
              "      <td>-0.143006</td>\n",
              "      <td>0.996951</td>\n",
              "      <td>0.525226</td>\n",
              "      <td>...</td>\n",
              "      <td>0.933201</td>\n",
              "      <td>0.517541</td>\n",
              "      <td>0.694255</td>\n",
              "      <td>0.512173</td>\n",
              "      <td>0.918859</td>\n",
              "      <td>0.522368</td>\n",
              "      <td>0.788383</td>\n",
              "      <td>0.123527</td>\n",
              "      <td>0.979749</td>\n",
              "      <td>lift</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>2.200000</td>\n",
              "      <td>0.528124</td>\n",
              "      <td>0.174302</td>\n",
              "      <td>-0.152541</td>\n",
              "      <td>0.998547</td>\n",
              "      <td>0.526977</td>\n",
              "      <td>0.154200</td>\n",
              "      <td>-0.142519</td>\n",
              "      <td>0.997176</td>\n",
              "      <td>0.526326</td>\n",
              "      <td>...</td>\n",
              "      <td>0.937389</td>\n",
              "      <td>0.517484</td>\n",
              "      <td>0.695698</td>\n",
              "      <td>0.490626</td>\n",
              "      <td>0.917648</td>\n",
              "      <td>0.522242</td>\n",
              "      <td>0.786339</td>\n",
              "      <td>0.155857</td>\n",
              "      <td>0.980697</td>\n",
              "      <td>lift</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>2.233333</td>\n",
              "      <td>0.529213</td>\n",
              "      <td>0.174701</td>\n",
              "      <td>-0.152821</td>\n",
              "      <td>0.998639</td>\n",
              "      <td>0.527567</td>\n",
              "      <td>0.154670</td>\n",
              "      <td>-0.142569</td>\n",
              "      <td>0.997375</td>\n",
              "      <td>0.526898</td>\n",
              "      <td>...</td>\n",
              "      <td>0.940666</td>\n",
              "      <td>0.517703</td>\n",
              "      <td>0.696621</td>\n",
              "      <td>0.491895</td>\n",
              "      <td>0.915618</td>\n",
              "      <td>0.521500</td>\n",
              "      <td>0.785614</td>\n",
              "      <td>0.169460</td>\n",
              "      <td>0.981178</td>\n",
              "      <td>lift</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3730</th>\n",
              "      <td>124.366667</td>\n",
              "      <td>0.577979</td>\n",
              "      <td>0.137515</td>\n",
              "      <td>-0.212173</td>\n",
              "      <td>0.998940</td>\n",
              "      <td>0.573762</td>\n",
              "      <td>0.120730</td>\n",
              "      <td>-0.202968</td>\n",
              "      <td>0.998647</td>\n",
              "      <td>0.572678</td>\n",
              "      <td>...</td>\n",
              "      <td>0.940920</td>\n",
              "      <td>0.557280</td>\n",
              "      <td>0.712589</td>\n",
              "      <td>0.506722</td>\n",
              "      <td>0.715824</td>\n",
              "      <td>0.558338</td>\n",
              "      <td>0.756399</td>\n",
              "      <td>0.292689</td>\n",
              "      <td>0.957541</td>\n",
              "      <td>place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3731</th>\n",
              "      <td>124.400000</td>\n",
              "      <td>0.577528</td>\n",
              "      <td>0.135466</td>\n",
              "      <td>-0.203692</td>\n",
              "      <td>0.998871</td>\n",
              "      <td>0.573423</td>\n",
              "      <td>0.119247</td>\n",
              "      <td>-0.193865</td>\n",
              "      <td>0.998545</td>\n",
              "      <td>0.572358</td>\n",
              "      <td>...</td>\n",
              "      <td>0.938307</td>\n",
              "      <td>0.557491</td>\n",
              "      <td>0.713130</td>\n",
              "      <td>0.491774</td>\n",
              "      <td>0.708402</td>\n",
              "      <td>0.558267</td>\n",
              "      <td>0.756366</td>\n",
              "      <td>0.279804</td>\n",
              "      <td>0.955889</td>\n",
              "      <td>place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3732</th>\n",
              "      <td>124.433333</td>\n",
              "      <td>0.576394</td>\n",
              "      <td>0.133642</td>\n",
              "      <td>-0.181532</td>\n",
              "      <td>0.998844</td>\n",
              "      <td>0.572369</td>\n",
              "      <td>0.117634</td>\n",
              "      <td>-0.170984</td>\n",
              "      <td>0.998499</td>\n",
              "      <td>0.571335</td>\n",
              "      <td>...</td>\n",
              "      <td>0.936846</td>\n",
              "      <td>0.557354</td>\n",
              "      <td>0.713610</td>\n",
              "      <td>0.480324</td>\n",
              "      <td>0.705018</td>\n",
              "      <td>0.558180</td>\n",
              "      <td>0.755997</td>\n",
              "      <td>0.260479</td>\n",
              "      <td>0.955023</td>\n",
              "      <td>place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3733</th>\n",
              "      <td>124.466667</td>\n",
              "      <td>0.576100</td>\n",
              "      <td>0.132289</td>\n",
              "      <td>-0.173792</td>\n",
              "      <td>0.998757</td>\n",
              "      <td>0.571993</td>\n",
              "      <td>0.116174</td>\n",
              "      <td>-0.163086</td>\n",
              "      <td>0.998367</td>\n",
              "      <td>0.570957</td>\n",
              "      <td>...</td>\n",
              "      <td>0.936236</td>\n",
              "      <td>0.557746</td>\n",
              "      <td>0.712463</td>\n",
              "      <td>0.480986</td>\n",
              "      <td>0.704123</td>\n",
              "      <td>0.558142</td>\n",
              "      <td>0.755949</td>\n",
              "      <td>0.254140</td>\n",
              "      <td>0.954824</td>\n",
              "      <td>place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3734</th>\n",
              "      <td>124.500000</td>\n",
              "      <td>0.574477</td>\n",
              "      <td>0.130638</td>\n",
              "      <td>-0.148313</td>\n",
              "      <td>0.998694</td>\n",
              "      <td>0.570444</td>\n",
              "      <td>0.114529</td>\n",
              "      <td>-0.137159</td>\n",
              "      <td>0.998269</td>\n",
              "      <td>0.569471</td>\n",
              "      <td>...</td>\n",
              "      <td>0.935421</td>\n",
              "      <td>0.557443</td>\n",
              "      <td>0.712672</td>\n",
              "      <td>0.459502</td>\n",
              "      <td>0.703168</td>\n",
              "      <td>0.558087</td>\n",
              "      <td>0.755887</td>\n",
              "      <td>0.235088</td>\n",
              "      <td>0.954480</td>\n",
              "      <td>place</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3646 rows Ã— 134 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            time  keypoint_0_x  keypoint_0_y  keypoint_0_z  \\\n",
              "62      2.100000      0.524635      0.171705     -0.151355   \n",
              "63      2.133333      0.525219      0.172805     -0.147992   \n",
              "64      2.166667      0.526411      0.173942     -0.152887   \n",
              "65      2.200000      0.528124      0.174302     -0.152541   \n",
              "66      2.233333      0.529213      0.174701     -0.152821   \n",
              "...          ...           ...           ...           ...   \n",
              "3730  124.366667      0.577979      0.137515     -0.212173   \n",
              "3731  124.400000      0.577528      0.135466     -0.203692   \n",
              "3732  124.433333      0.576394      0.133642     -0.181532   \n",
              "3733  124.466667      0.576100      0.132289     -0.173792   \n",
              "3734  124.500000      0.574477      0.130638     -0.148313   \n",
              "\n",
              "      keypoint_0_visibility  keypoint_1_x  keypoint_1_y  keypoint_1_z  \\\n",
              "62                 0.998253      0.523957      0.151215     -0.140882   \n",
              "63                 0.998362      0.524634      0.151772     -0.138045   \n",
              "64                 0.998442      0.525811      0.153505     -0.143006   \n",
              "65                 0.998547      0.526977      0.154200     -0.142519   \n",
              "66                 0.998639      0.527567      0.154670     -0.142569   \n",
              "...                     ...           ...           ...           ...   \n",
              "3730               0.998940      0.573762      0.120730     -0.202968   \n",
              "3731               0.998871      0.573423      0.119247     -0.193865   \n",
              "3732               0.998844      0.572369      0.117634     -0.170984   \n",
              "3733               0.998757      0.571993      0.116174     -0.163086   \n",
              "3734               0.998694      0.570444      0.114529     -0.137159   \n",
              "\n",
              "      keypoint_1_visibility  keypoint_2_x  ...  keypoint_30_visibility  \\\n",
              "62                 0.996524      0.523311  ...                0.924650   \n",
              "63                 0.996766      0.524023  ...                0.929728   \n",
              "64                 0.996951      0.525226  ...                0.933201   \n",
              "65                 0.997176      0.526326  ...                0.937389   \n",
              "66                 0.997375      0.526898  ...                0.940666   \n",
              "...                     ...           ...  ...                     ...   \n",
              "3730               0.998647      0.572678  ...                0.940920   \n",
              "3731               0.998545      0.572358  ...                0.938307   \n",
              "3732               0.998499      0.571335  ...                0.936846   \n",
              "3733               0.998367      0.570957  ...                0.936236   \n",
              "3734               0.998269      0.569471  ...                0.935421   \n",
              "\n",
              "      keypoint_31_x  keypoint_31_y  keypoint_31_z  keypoint_31_visibility  \\\n",
              "62         0.515694       0.698783       0.477038                0.924965   \n",
              "63         0.516461       0.694523       0.503850                0.921965   \n",
              "64         0.517541       0.694255       0.512173                0.918859   \n",
              "65         0.517484       0.695698       0.490626                0.917648   \n",
              "66         0.517703       0.696621       0.491895                0.915618   \n",
              "...             ...            ...            ...                     ...   \n",
              "3730       0.557280       0.712589       0.506722                0.715824   \n",
              "3731       0.557491       0.713130       0.491774                0.708402   \n",
              "3732       0.557354       0.713610       0.480324                0.705018   \n",
              "3733       0.557746       0.712463       0.480986                0.704123   \n",
              "3734       0.557443       0.712672       0.459502                0.703168   \n",
              "\n",
              "      keypoint_32_x  keypoint_32_y  keypoint_32_z  keypoint_32_visibility  \\\n",
              "62         0.523433       0.786935       0.077680                0.978275   \n",
              "63         0.522602       0.784162       0.121329                0.979272   \n",
              "64         0.522368       0.788383       0.123527                0.979749   \n",
              "65         0.522242       0.786339       0.155857                0.980697   \n",
              "66         0.521500       0.785614       0.169460                0.981178   \n",
              "...             ...            ...            ...                     ...   \n",
              "3730       0.558338       0.756399       0.292689                0.957541   \n",
              "3731       0.558267       0.756366       0.279804                0.955889   \n",
              "3732       0.558180       0.755997       0.260479                0.955023   \n",
              "3733       0.558142       0.755949       0.254140                0.954824   \n",
              "3734       0.558087       0.755887       0.235088                0.954480   \n",
              "\n",
              "      label  \n",
              "62     lift  \n",
              "63     lift  \n",
              "64     lift  \n",
              "65     lift  \n",
              "66     lift  \n",
              "...     ...  \n",
              "3730  place  \n",
              "3731  place  \n",
              "3732  place  \n",
              "3733  place  \n",
              "3734  place  \n",
              "\n",
              "[3646 rows x 134 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: df /content/Labeled_Dataset__English_.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(r'D:\\git_project\\workshop_lifting\\app\\Train\\Labeled_Dataset__English_.csv')\n",
        "df_with_unk = df[df['label']!= 'unknown']\n",
        "df_with_unk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "O-MHnFyvWlUa",
        "outputId": "fd745a29-fbda-47ce-fb7b-50d2d35805d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "time        keypoint_0_x  keypoint_0_y  keypoint_0_z  keypoint_0_visibility  keypoint_1_x  keypoint_1_y  keypoint_1_z  keypoint_1_visibility  keypoint_2_x  keypoint_2_y  keypoint_2_z  keypoint_2_visibility  keypoint_3_x  keypoint_3_y  keypoint_3_z  keypoint_3_visibility  keypoint_4_x  keypoint_4_y  keypoint_4_z  keypoint_4_visibility  keypoint_5_x  keypoint_5_y  keypoint_5_z  keypoint_5_visibility  keypoint_6_x  keypoint_6_y  keypoint_6_z  keypoint_6_visibility  keypoint_7_x  keypoint_7_y  keypoint_7_z  keypoint_7_visibility  keypoint_8_x  keypoint_8_y  keypoint_8_z  keypoint_8_visibility  keypoint_9_x  keypoint_9_y  keypoint_9_z  keypoint_9_visibility  keypoint_10_x  keypoint_10_y  keypoint_10_z  keypoint_10_visibility  keypoint_11_x  keypoint_11_y  keypoint_11_z  keypoint_11_visibility  keypoint_12_x  keypoint_12_y  keypoint_12_z  keypoint_12_visibility  keypoint_13_x  keypoint_13_y  keypoint_13_z  keypoint_13_visibility  keypoint_14_x  keypoint_14_y  keypoint_14_z  keypoint_14_visibility  keypoint_15_x  keypoint_15_y  keypoint_15_z  keypoint_15_visibility  keypoint_16_x  keypoint_16_y  keypoint_16_z  keypoint_16_visibility  keypoint_17_x  keypoint_17_y  keypoint_17_z  keypoint_17_visibility  keypoint_18_x  keypoint_18_y  keypoint_18_z  keypoint_18_visibility  keypoint_19_x  keypoint_19_y  keypoint_19_z  keypoint_19_visibility  keypoint_20_x  keypoint_20_y  keypoint_20_z  keypoint_20_visibility  keypoint_21_x  keypoint_21_y  keypoint_21_z  keypoint_21_visibility  keypoint_22_x  keypoint_22_y  keypoint_22_z  keypoint_22_visibility  keypoint_23_x  keypoint_23_y  keypoint_23_z  keypoint_23_visibility  keypoint_24_x  keypoint_24_y  keypoint_24_z  keypoint_24_visibility  keypoint_25_x  keypoint_25_y  keypoint_25_z  keypoint_25_visibility  keypoint_26_x  keypoint_26_y  keypoint_26_z  keypoint_26_visibility  keypoint_27_x  keypoint_27_y  keypoint_27_z  keypoint_27_visibility  keypoint_28_x  keypoint_28_y  keypoint_28_z  keypoint_28_visibility  keypoint_29_x  keypoint_29_y  keypoint_29_z  keypoint_29_visibility  keypoint_30_x  keypoint_30_y  keypoint_30_z  keypoint_30_visibility  keypoint_31_x  keypoint_31_y  keypoint_31_z  keypoint_31_visibility  keypoint_32_x  keypoint_32_y  keypoint_32_z  keypoint_32_visibility  label\n",
              "2.100000    0.524635      0.171705      -0.151355     0.998253               0.523957      0.151215      -0.140882     0.996524               0.523311      0.150480      -0.140932     0.996368               0.522531      0.149693      -0.140947     0.996518               0.522848      0.150307      -0.175152     0.997617               0.521525      0.148987      -0.175198     0.997724               0.520009      0.147543      -0.175253     0.997861               0.509271      0.149115      -0.076271     0.989258               0.507130      0.147360      -0.233203     0.997786               0.519480      0.183294      -0.122326     0.996153               0.517855       0.182838       -0.167329      0.998030                0.486788       0.222152        0.033705      0.998647                0.490518       0.246793       -0.287772      0.999907                0.500014       0.344973        0.096510      0.106052                0.517333       0.362236       -0.290712      0.991853                0.542660       0.395924        0.023364      0.125680                0.572412       0.333734       -0.231284      0.976841                0.554176       0.403465        0.015143      0.141849                0.589577       0.334774       -0.260832      0.960758                0.557597       0.396499       -0.008428      0.149992                0.590764       0.322610       -0.257225      0.959596                0.553306       0.395603        0.010198      0.155315                0.585958       0.320556       -0.228375      0.925485                0.485072       0.439034        0.099912      0.997333                0.486558       0.451019       -0.099969      0.998722                0.493688       0.569184        0.287253      0.613594                0.497360       0.603879        0.015998      0.973114                0.480936       0.687646       0.485893       0.819798                0.490165       0.749792        0.150199      0.981662                0.470601       0.706608       0.503809       0.823569                0.477810       0.777792        0.160486      0.924650                0.515694       0.698783       0.477038       0.924965                0.523433       0.786935        0.077680      0.978275                lift     1\n",
              "84.033333   0.277174      0.335571      -0.483582     0.999810               0.281041      0.326991      -0.491878     0.999831               0.284922      0.323727      -0.491875     0.999833               0.288808      0.320211      -0.492010     0.999872               0.271347      0.329773      -0.488520     0.999709               0.268322      0.328776      -0.488570     0.999605               0.265189      0.327696      -0.488600     0.999673               0.295251      0.298706      -0.446195     0.999855               0.259855      0.308709      -0.430169     0.999299               0.284345      0.328948      -0.457449     0.999882               0.272157       0.331773       -0.453071      0.999780                0.334247       0.290819       -0.370919      0.999983                0.239368       0.307495       -0.325650      0.999927                0.343161       0.433481       -0.338562      0.989766                0.230190       0.436111       -0.282184      0.880191                0.353501       0.568187       -0.357764      0.965173                0.215653       0.573201       -0.305742      0.898411                0.361086       0.601918       -0.385375      0.914518                0.209735       0.605076       -0.324864      0.803715                0.350358       0.602483       -0.402682      0.934743                0.216779       0.606810       -0.352077      0.850276                0.345741       0.593531       -0.365597      0.922417                0.219846       0.598626       -0.317311      0.844546                0.319938       0.419346       -0.016670      0.999860                0.272324       0.424286        0.016765      0.999679                0.328906       0.536578        0.065416      0.530115                0.286057       0.545933        0.123668      0.382081                0.326175       0.504961       0.353380       0.058068                0.298477       0.510874        0.393214      0.049783                0.326890       0.492043       0.385494       0.062080                0.302136       0.502625        0.422970      0.060933                0.321596       0.526628       0.387399       0.136482                0.296414       0.531357        0.426083      0.136032                lift     1\n",
              "83.166667   0.264949      0.302304      -0.563563     0.999363               0.269024      0.290589      -0.570466     0.999117               0.273031      0.286572      -0.570439     0.999163               0.277081      0.282030      -0.570585     0.999324               0.258970      0.293799      -0.563323     0.998608               0.255917      0.292465      -0.563406     0.998230               0.252724      0.290933      -0.563470     0.998355               0.282932      0.260982      -0.508329     0.999220               0.247912      0.273441      -0.472088     0.996792               0.273346      0.298030      -0.529843     0.999578               0.260799       0.303352       -0.519888      0.998993                0.325107       0.261444       -0.424279      0.999982                0.229521       0.291338       -0.324802      0.999938                0.352334       0.394766       -0.401121      0.990222                0.221432       0.418319       -0.239018      0.908006                0.360424       0.535517       -0.441459      0.950311                0.215302       0.557405       -0.267124      0.906869                0.367175       0.578772       -0.467644      0.897275                0.209643       0.595440       -0.275199      0.854547                0.358270       0.581502       -0.488605      0.911935                0.214455       0.597203       -0.312132      0.875225                0.353082       0.568537       -0.449132      0.900898                0.218260       0.585349       -0.281042      0.872410                0.323559       0.439597       -0.037483      0.999525                0.272942       0.453237        0.037490      0.999154                0.327279       0.605855       -0.071263      0.600622                0.288238       0.611683        0.057450      0.413453                0.317221       0.524657       0.293618       0.082276                0.283724       0.542237        0.388352      0.056398                0.317680       0.494193       0.340574       0.109333                0.285654       0.524634        0.428017      0.087435                0.314803       0.552511       0.385531       0.126342                0.272770       0.556811        0.485493      0.096750                lift     1\n",
              "83.200000   0.265317      0.305583      -0.564009     0.999406               0.269442      0.294150      -0.571940     0.999183               0.273430      0.290252      -0.571904     0.999226               0.277481      0.285872      -0.572055     0.999374               0.259411      0.297383      -0.564571     0.998710               0.256403      0.296182      -0.564649     0.998361               0.253277      0.294863      -0.564707     0.998474               0.283672      0.265384      -0.512230     0.999279               0.248975      0.278214      -0.475820     0.997007               0.273654      0.301766      -0.531262     0.999610               0.260991       0.306880       -0.521163      0.999073                0.325168       0.266531       -0.422615      0.999983                0.230243       0.294124       -0.325788      0.999941                0.351005       0.397901       -0.392733      0.990353                0.222859       0.419054       -0.236203      0.911733                0.359602       0.540069       -0.429099      0.951940                0.215648       0.559478       -0.254070      0.911264                0.366335       0.583529       -0.456480      0.900342                0.209688       0.596137       -0.258732      0.859650                0.356122       0.586273       -0.478846      0.914819                0.214852       0.598193       -0.294136      0.880063                0.351071       0.575842       -0.437466      0.903932                0.218759       0.586823       -0.266960      0.877569                0.322993       0.435181       -0.036782      0.999571                0.273036       0.448276        0.036775      0.999236                0.327885       0.607212       -0.061728      0.619446                0.290129       0.610840        0.064236      0.430538                0.318937       0.516898       0.283967       0.081667                0.285937       0.534566        0.385950      0.055247                0.319519       0.487246       0.325584       0.107127                0.288199       0.517067        0.426602      0.085870                0.316020       0.539031       0.377758       0.126196                0.278381       0.543930        0.464843      0.095713                lift     1\n",
              "83.233333   0.267339      0.308441      -0.561453     0.999441               0.271360      0.296756      -0.569729     0.999243               0.275272      0.292603      -0.569710     0.999281               0.279224      0.288023      -0.569869     0.999419               0.261307      0.300473      -0.561875     0.998796               0.258295      0.299404      -0.561947     0.998464               0.255246      0.298241      -0.562003     0.998575               0.284535      0.267230      -0.511300     0.999332               0.250855      0.281513      -0.472476     0.997169               0.275446      0.303991      -0.529002     0.999640               0.262639       0.308978       -0.518326      0.999146                0.325159       0.268912       -0.426613      0.999984                0.231008       0.293068       -0.321254      0.999944                0.348495       0.404725       -0.397387      0.990151                0.223978       0.419819       -0.230479      0.914390                0.357824       0.544548       -0.437423      0.952041                0.215738       0.561499       -0.253179      0.913957                0.364976       0.589414       -0.464432      0.901188                0.209850       0.597725       -0.258609      0.861930                0.355250       0.592550       -0.485872      0.915813                0.214996       0.600626       -0.295650      0.882452                0.350146       0.583995       -0.445446      0.904662                0.218840       0.588528       -0.266888      0.880346                0.322679       0.433382       -0.038138      0.999613                0.273094       0.446269        0.038184      0.999308                0.316402       0.607555       -0.065032      0.640651                0.290124       0.601405        0.091168      0.441749                0.319889       0.521898       0.295412       0.081484                0.287669       0.531270        0.410636      0.054429                0.320530       0.494427       0.337540       0.105050                0.289299       0.517654        0.448554      0.084794                0.316585       0.549058       0.381261       0.126109                0.284191       0.540988        0.485841      0.095169                lift     1\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ..\n",
              "42.933333   0.553933      0.252953      -0.327324     0.999025               0.559737      0.236159      -0.317897     0.998095               0.562816      0.235580      -0.317881     0.997857               0.565885      0.234959      -0.317833     0.998191               0.550211      0.234537      -0.319466     0.998737               0.546925      0.233094      -0.319476     0.998685               0.544273      0.231804      -0.319482     0.998967               0.571199      0.232307      -0.219260     0.994581               0.539787      0.228228      -0.231553     0.998777               0.558709      0.264880      -0.287006     0.998423               0.547436       0.263275       -0.290389      0.999267                0.591035       0.315629       -0.116460      0.999134                0.509381       0.312483       -0.165609      0.999981                0.596968       0.425793       -0.060126      0.414046                0.496423       0.431231       -0.144145      0.993199                0.604350       0.526472       -0.145508      0.430016                0.496643       0.542441       -0.212621      0.977836                0.606098       0.557227       -0.169148      0.424700                0.494705       0.579854       -0.239489      0.959458                0.602000       0.557835       -0.198976      0.430691                0.502727       0.579130       -0.261935      0.958495                0.599855       0.548375       -0.158515      0.422990                0.504877       0.567945       -0.221508      0.937503                0.555782       0.536815        0.021468      0.998332                0.515638       0.533553       -0.021619      0.999582                0.540886       0.678141        0.176895      0.660645                0.500924       0.679293       -0.047179      0.982811                0.539612       0.784175       0.374432       0.782182                0.489245       0.818836        0.008966      0.982694                0.528619       0.805578       0.389642       0.796291                0.493044       0.834875        0.008818      0.909048                0.569028       0.816626       0.319921       0.882068                0.477968       0.876906       -0.106753      0.975182                empty    1\n",
              "42.966667   0.548745      0.253295      -0.343635     0.999122               0.555055      0.236580      -0.337512     0.998283               0.558560      0.235949      -0.337473     0.998068               0.561880      0.235252      -0.337429     0.998367               0.545677      0.235313      -0.336602     0.998861               0.543104      0.233996      -0.336628     0.998815               0.540595      0.232729      -0.336622     0.999068               0.568613      0.232146      -0.243738     0.995110               0.538072      0.228509      -0.238775     0.998896               0.554723      0.264876      -0.304010     0.998579               0.543361       0.263609       -0.303718      0.999339                0.591537       0.313985       -0.139261      0.999218                0.508175       0.309869       -0.166701      0.999982                0.599419       0.424118       -0.079061      0.471660                0.494339       0.427305       -0.132491      0.992814                0.607141       0.526611       -0.154509      0.484926                0.491236       0.538204       -0.196546      0.976647                0.608672       0.557027       -0.175186      0.476502                0.488188       0.575295       -0.220443      0.956310                0.604903       0.557705       -0.206535      0.482421                0.494896       0.574866       -0.245233      0.955978                0.603178       0.548390       -0.167571      0.474620                0.497639       0.564002       -0.206540      0.935164                0.556867       0.534109        0.011890      0.998474                0.514608       0.528705       -0.012004      0.999622                0.542054       0.676838        0.155626      0.692797                0.500089       0.677271       -0.044523      0.984325                0.541081       0.781914       0.351241       0.802045                0.486623       0.817560        0.014259      0.984080                0.530275       0.805609       0.366108       0.811865                0.492230       0.836042        0.013766      0.912347                0.568585       0.815630       0.289352       0.891967                0.467354       0.867871       -0.101712      0.976978                empty    1\n",
              "43.000000   0.544341      0.249367      -0.336497     0.999209               0.550555      0.232469      -0.330367     0.998449               0.554265      0.231794      -0.330336     0.998255               0.557865      0.230989      -0.330302     0.998522               0.541826      0.231866      -0.329302     0.998972               0.539370      0.230664      -0.329336     0.998930               0.537297      0.229439      -0.329345     0.999159               0.565293      0.228225      -0.242115     0.995577               0.536367      0.224563      -0.235387     0.998999               0.550564      0.260084      -0.299022     0.998718               0.539579       0.259749       -0.298392      0.999404                0.591189       0.312805       -0.146948      0.999290                0.506779       0.308185       -0.155474      0.999984                0.601978       0.420122       -0.093932      0.523326                0.492946       0.424148       -0.116954      0.991869                0.611402       0.525119       -0.161109      0.532614                0.485531       0.534291       -0.179723      0.974046                0.614446       0.555515       -0.180159      0.518638                0.481216       0.569748       -0.201769      0.950598                0.611022       0.556982       -0.209930      0.524883                0.487080       0.569991       -0.228078      0.951014                0.609347       0.547948       -0.173530      0.516819                0.490214       0.560136       -0.190705      0.929917                0.556828       0.531173        0.003540      0.998592                0.513099       0.524322       -0.003627      0.999658                0.543261       0.672432        0.142315      0.721593                0.499716       0.672878       -0.048993      0.985699                0.543427       0.778073       0.338569       0.819536                0.485551       0.815493       -0.003013      0.985356                0.532529       0.803463       0.353409       0.823864                0.491936       0.836737       -0.005217      0.915730                0.569824       0.811235       0.277003       0.900616                0.462124       0.860956       -0.119662      0.978692                empty    1\n",
              "43.033333   0.544119      0.246630      -0.331663     0.999288               0.550171      0.228921      -0.325160     0.998602               0.553887      0.227926      -0.325123     0.998427               0.557345      0.226846      -0.325086     0.998666               0.541367      0.229160      -0.325087     0.999074               0.538787      0.228201      -0.325122     0.999036               0.536631      0.227213      -0.325134     0.999241               0.563868      0.224452      -0.237894     0.996011               0.535182      0.222691      -0.232861     0.999096               0.550302      0.257144      -0.294553     0.998845               0.539178       0.257699       -0.295554      0.999462                0.590739       0.310606       -0.145452      0.999358                0.505383       0.307328       -0.153610      0.999985                0.603414       0.416924       -0.090666      0.570494                0.490940       0.421494       -0.114702      0.991296                0.615313       0.524595       -0.158324      0.577991                0.481883       0.530301       -0.177851      0.972950                0.620312       0.555004       -0.177439      0.562517                0.476848       0.564745       -0.199785      0.947829                0.615974       0.556343       -0.208873      0.568614                0.481892       0.564912       -0.227623      0.948896                0.614056       0.547431       -0.171542      0.560837                0.485000       0.555569       -0.189803      0.928199                0.556766       0.527524        0.002534      0.998709                0.512087       0.520391       -0.002603      0.999691                0.544962       0.665367        0.139611      0.748100                0.500391       0.670033       -0.045207      0.987003                0.544407       0.771140       0.337042       0.835701                0.485471       0.812874       -0.000690      0.986603                0.533588       0.798676       0.352012       0.835436                0.492197       0.836402       -0.003356      0.920142                0.569744       0.804330       0.279961       0.908713                0.458380       0.854809       -0.113144      0.980378                empty    1\n",
              "124.500000  0.574477      0.130638      -0.148313     0.998694               0.570444      0.114529      -0.137159     0.998269               0.569471      0.113864      -0.137220     0.998118               0.568363      0.113156      -0.137284     0.998617               0.570087      0.115149      -0.170803     0.998510               0.568962      0.115075      -0.170807     0.998348               0.567707      0.114961      -0.170842     0.998526               0.555799      0.116741      -0.071743     0.996771               0.555257      0.119962      -0.223765     0.997989               0.569604      0.145516      -0.119073     0.997490               0.568776       0.146199       -0.163031      0.998086                0.528398       0.201008        0.031614      0.998607                0.539343       0.228919       -0.269713      0.999714                0.533005       0.314139        0.117521      0.017223                0.537033       0.368391       -0.292070      0.981973                0.587276       0.339920        0.089123      0.076401                0.611036       0.353203       -0.253807      0.970945                0.594261       0.330980        0.083523      0.132704                0.627387       0.357059       -0.287898      0.960107                0.599808       0.304871        0.057609      0.143210                0.627186       0.336836       -0.279234      0.955617                0.594551       0.308418        0.075366      0.153787                0.621229       0.335253       -0.250094      0.910228                0.534495       0.422927        0.094147      0.993615                0.541992       0.445665       -0.094178      0.996601                0.534710       0.565950        0.280209      0.150715                0.532254       0.591076        0.078126      0.910212                0.521988       0.693179       0.471306       0.384633                0.517460       0.735095        0.274449      0.971447                0.512439       0.722461       0.488965       0.567283                0.504913       0.767359        0.292201      0.935421                0.557443       0.712672       0.459502       0.703168                0.558087       0.755887        0.235088      0.954480                place    1\n",
              "Name: count, Length: 3646, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_with_unk.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37PpNT5nbaly",
        "outputId": "4c73fe48-103c-41f2-8242-921cefd4b921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['time', 'keypoint_0_x', 'keypoint_0_y', 'keypoint_0_z', 'keypoint_0_visibility', 'keypoint_1_x', 'keypoint_1_y', 'keypoint_1_z', 'keypoint_1_visibility', 'keypoint_2_x', 'keypoint_2_y', 'keypoint_2_z', 'keypoint_2_visibility', 'keypoint_3_x', 'keypoint_3_y', 'keypoint_3_z', 'keypoint_3_visibility', 'keypoint_4_x', 'keypoint_4_y', 'keypoint_4_z', 'keypoint_4_visibility', 'keypoint_5_x', 'keypoint_5_y', 'keypoint_5_z', 'keypoint_5_visibility', 'keypoint_6_x', 'keypoint_6_y', 'keypoint_6_z', 'keypoint_6_visibility', 'keypoint_7_x', 'keypoint_7_y', 'keypoint_7_z', 'keypoint_7_visibility', 'keypoint_8_x', 'keypoint_8_y', 'keypoint_8_z', 'keypoint_8_visibility', 'keypoint_9_x', 'keypoint_9_y', 'keypoint_9_z', 'keypoint_9_visibility', 'keypoint_10_x', 'keypoint_10_y', 'keypoint_10_z', 'keypoint_10_visibility', 'keypoint_11_x', 'keypoint_11_y', 'keypoint_11_z', 'keypoint_11_visibility', 'keypoint_12_x', 'keypoint_12_y', 'keypoint_12_z', 'keypoint_12_visibility', 'keypoint_13_x', 'keypoint_13_y', 'keypoint_13_z', 'keypoint_13_visibility', 'keypoint_14_x', 'keypoint_14_y', 'keypoint_14_z', 'keypoint_14_visibility', 'keypoint_15_x', 'keypoint_15_y', 'keypoint_15_z', 'keypoint_15_visibility', 'keypoint_16_x', 'keypoint_16_y', 'keypoint_16_z', 'keypoint_16_visibility', 'keypoint_17_x', 'keypoint_17_y', 'keypoint_17_z', 'keypoint_17_visibility', 'keypoint_18_x', 'keypoint_18_y', 'keypoint_18_z', 'keypoint_18_visibility', 'keypoint_19_x', 'keypoint_19_y', 'keypoint_19_z', 'keypoint_19_visibility', 'keypoint_20_x', 'keypoint_20_y', 'keypoint_20_z', 'keypoint_20_visibility', 'keypoint_21_x', 'keypoint_21_y', 'keypoint_21_z', 'keypoint_21_visibility', 'keypoint_22_x', 'keypoint_22_y', 'keypoint_22_z', 'keypoint_22_visibility', 'keypoint_23_x', 'keypoint_23_y', 'keypoint_23_z', 'keypoint_23_visibility', 'keypoint_24_x', 'keypoint_24_y', 'keypoint_24_z', 'keypoint_24_visibility', 'keypoint_25_x', 'keypoint_25_y', 'keypoint_25_z', 'keypoint_25_visibility', 'keypoint_26_x', 'keypoint_26_y', 'keypoint_26_z', 'keypoint_26_visibility', 'keypoint_27_x', 'keypoint_27_y', 'keypoint_27_z', 'keypoint_27_visibility', 'keypoint_28_x', 'keypoint_28_y', 'keypoint_28_z', 'keypoint_28_visibility', 'keypoint_29_x', 'keypoint_29_y', 'keypoint_29_z', 'keypoint_29_visibility', 'keypoint_30_x', 'keypoint_30_y', 'keypoint_30_z', 'keypoint_30_visibility', 'keypoint_31_x', 'keypoint_31_y', 'keypoint_31_z', 'keypoint_31_visibility', 'keypoint_32_x', 'keypoint_32_y', 'keypoint_32_z', 'keypoint_32_visibility', 'label']\n"
          ]
        }
      ],
      "source": [
        "# prompt: df show colums all\n",
        "\n",
        "print(df_with_unk.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP6gr4e3a-l6",
        "outputId": "463f0531-dfe5-42c2-a93e-fe393f00637c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3646 entries, 62 to 3734\n",
            "Columns: 134 entries, time to label\n",
            "dtypes: float64(133), object(1)\n",
            "memory usage: 3.8+ MB\n"
          ]
        }
      ],
      "source": [
        "df_with_unk.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWex1pHMZzDK",
        "outputId": "9405409a-a79d-46e1-bb62-ba09a1c973de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Distribution:\n",
            "Class empty: 1192 samples\n",
            "Class hold: 1445 samples\n",
            "Class lift: 579 samples\n",
            "Class place: 430 samples\n",
            "Class unknown: 229 samples\n",
            "Class Weights:\n",
            "Class 0: 0.0842\n",
            "Class 1: 0.0695\n",
            "Class 2: 0.1736\n",
            "Class 3: 0.2336\n",
            "Class 4: 0.4391\n",
            "Epoch [1/50], Training Loss: 1.1119\n",
            "Test Accuracy: 0.3742\n",
            "Epoch [2/50], Training Loss: 0.7440\n",
            "Test Accuracy: 0.5135\n",
            "Epoch [3/50], Training Loss: 0.5278\n",
            "Test Accuracy: 0.6723\n",
            "Epoch [4/50], Training Loss: 0.4507\n",
            "Test Accuracy: 0.7523\n",
            "Epoch [5/50], Training Loss: 0.3796\n",
            "Test Accuracy: 0.7768\n",
            "Epoch [6/50], Training Loss: 0.3121\n",
            "Test Accuracy: 0.7858\n",
            "Epoch [7/50], Training Loss: 0.2959\n",
            "Test Accuracy: 0.8284\n",
            "Epoch [8/50], Training Loss: 0.2289\n",
            "Test Accuracy: 0.8581\n",
            "Epoch [9/50], Training Loss: 0.2296\n",
            "Test Accuracy: 0.8710\n",
            "Epoch [10/50], Training Loss: 0.2037\n",
            "Test Accuracy: 0.8929\n",
            "Epoch [11/50], Training Loss: 0.1923\n",
            "Test Accuracy: 0.8916\n",
            "Epoch [12/50], Training Loss: 0.1680\n",
            "Test Accuracy: 0.8890\n",
            "Epoch [13/50], Training Loss: 0.1638\n",
            "Test Accuracy: 0.9265\n",
            "Epoch [14/50], Training Loss: 0.1811\n",
            "Test Accuracy: 0.9135\n",
            "Epoch [15/50], Training Loss: 0.1498\n",
            "Test Accuracy: 0.9097\n",
            "Epoch [16/50], Training Loss: 0.1346\n",
            "Test Accuracy: 0.9394\n",
            "Epoch [17/50], Training Loss: 0.1646\n",
            "Test Accuracy: 0.9213\n",
            "Epoch [18/50], Training Loss: 0.1451\n",
            "Test Accuracy: 0.9123\n",
            "Epoch [19/50], Training Loss: 0.1353\n",
            "Test Accuracy: 0.9239\n",
            "Epoch [20/50], Training Loss: 0.1182\n",
            "Test Accuracy: 0.9445\n",
            "Epoch [21/50], Training Loss: 0.1173\n",
            "Test Accuracy: 0.9277\n",
            "Epoch [22/50], Training Loss: 0.1106\n",
            "Test Accuracy: 0.9290\n",
            "Epoch [23/50], Training Loss: 0.1208\n",
            "Test Accuracy: 0.9239\n",
            "Epoch [24/50], Training Loss: 0.1099\n",
            "Test Accuracy: 0.9497\n",
            "Epoch [25/50], Training Loss: 0.1009\n",
            "Test Accuracy: 0.9265\n",
            "Epoch [26/50], Training Loss: 0.1004\n",
            "Test Accuracy: 0.9419\n",
            "Epoch [27/50], Training Loss: 0.1165\n",
            "Test Accuracy: 0.9458\n",
            "Epoch [28/50], Training Loss: 0.0990\n",
            "Test Accuracy: 0.9510\n",
            "Epoch [29/50], Training Loss: 0.0992\n",
            "Test Accuracy: 0.9497\n",
            "Epoch [30/50], Training Loss: 0.0877\n",
            "Test Accuracy: 0.9587\n",
            "Epoch [31/50], Training Loss: 0.0820\n",
            "Test Accuracy: 0.9471\n",
            "Epoch [32/50], Training Loss: 0.0879\n",
            "Test Accuracy: 0.9497\n",
            "Epoch [33/50], Training Loss: 0.0904\n",
            "Test Accuracy: 0.9690\n",
            "Epoch [34/50], Training Loss: 0.0782\n",
            "Test Accuracy: 0.9458\n",
            "Epoch [35/50], Training Loss: 0.0802\n",
            "Test Accuracy: 0.9665\n",
            "Epoch [36/50], Training Loss: 0.0665\n",
            "Test Accuracy: 0.9458\n",
            "Epoch [37/50], Training Loss: 0.0846\n",
            "Test Accuracy: 0.9561\n",
            "Epoch [38/50], Training Loss: 0.0955\n",
            "Test Accuracy: 0.9277\n",
            "Epoch [39/50], Training Loss: 0.0852\n",
            "Test Accuracy: 0.9535\n",
            "Epoch [40/50], Training Loss: 0.0607\n",
            "Test Accuracy: 0.9587\n",
            "Epoch [41/50], Training Loss: 0.0811\n",
            "Test Accuracy: 0.9523\n",
            "Epoch [42/50], Training Loss: 0.0650\n",
            "Test Accuracy: 0.9471\n",
            "Epoch [43/50], Training Loss: 0.0649\n",
            "Test Accuracy: 0.9613\n",
            "Epoch [44/50], Training Loss: 0.0558\n",
            "Test Accuracy: 0.9639\n",
            "Epoch [45/50], Training Loss: 0.0722\n",
            "Test Accuracy: 0.9652\n",
            "Epoch [46/50], Training Loss: 0.0830\n",
            "Test Accuracy: 0.9445\n",
            "Epoch [47/50], Training Loss: 0.0681\n",
            "Test Accuracy: 0.9484\n",
            "Epoch [48/50], Training Loss: 0.0771\n",
            "Test Accuracy: 0.9639\n",
            "Epoch [49/50], Training Loss: 0.0710\n",
            "Test Accuracy: 0.9613\n",
            "Epoch [50/50], Training Loss: 0.0700\n",
            "Test Accuracy: 0.9639\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       empty       1.00      0.96      0.98       238\n",
            "        hold       1.00      0.96      0.98       289\n",
            "        lift       0.92      0.98      0.95       116\n",
            "       place       0.86      0.99      0.92        86\n",
            "     unknown       0.96      0.96      0.96        46\n",
            "\n",
            "    accuracy                           0.96       775\n",
            "   macro avg       0.95      0.97      0.96       775\n",
            "weighted avg       0.97      0.96      0.96       775\n",
            "\n",
            "\n",
            "Model saved as 'keypoints_cnn_model.pth'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "class ImbalancedDatasetHandler:\n",
        "    @staticmethod\n",
        "    def analyze_class_distribution(labels):\n",
        "        \"\"\"\n",
        "        Analyze and visualize class distribution\n",
        "\n",
        "        Args:\n",
        "        labels (array-like): Array of labels\n",
        "\n",
        "        Returns:\n",
        "        dict: Class distribution information\n",
        "        \"\"\"\n",
        "        # Count class occurrences\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        class_dist = dict(zip(unique, counts))\n",
        "\n",
        "        # Visualize class distribution\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(unique, counts)\n",
        "        plt.title('Class Distribution')\n",
        "        plt.xlabel('Classes')\n",
        "        plt.ylabel('Number of Samples')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('class_distribution.png')\n",
        "        plt.close()\n",
        "\n",
        "        print(\"Class Distribution:\")\n",
        "        for cls, count in class_dist.items():\n",
        "            print(f\"Class {cls}: {count} samples\")\n",
        "\n",
        "        return class_dist\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_class_weights(labels):\n",
        "        \"\"\"\n",
        "        Compute weights for each class to balance the dataset\n",
        "\n",
        "        Args:\n",
        "        labels (array-like): Array of labels\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: Class weights\n",
        "        \"\"\"\n",
        "        # Count class occurrences\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "        # Compute weights (inverse of frequency)\n",
        "        weights = 1.0 / counts\n",
        "\n",
        "        # Normalize weights\n",
        "        weights = weights / np.sum(weights)\n",
        "\n",
        "        # Convert to torch tensor\n",
        "        class_weights = torch.FloatTensor(weights)\n",
        "\n",
        "        print(\"Class Weights:\")\n",
        "        for cls, weight in zip(unique, weights):\n",
        "            print(f\"Class {cls}: {weight:.4f}\")\n",
        "\n",
        "        return class_weights\n",
        "\n",
        "    @staticmethod\n",
        "    def create_weighted_sampler(labels):\n",
        "        \"\"\"\n",
        "        Create a weighted sampler to balance dataset during training\n",
        "\n",
        "        Args:\n",
        "        labels (array-like): Array of labels\n",
        "\n",
        "        Returns:\n",
        "        WeightedRandomSampler: Sampler that balances class distribution\n",
        "        \"\"\"\n",
        "        # Count class occurrences\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "        # Compute sample weights\n",
        "        sample_weights = np.zeros(len(labels))\n",
        "        for cls in unique:\n",
        "            # Find indices of current class\n",
        "            cls_indices = np.where(labels == cls)[0]\n",
        "\n",
        "            # Compute weight for this class\n",
        "            cls_weight = 1.0 / counts[np.where(unique == cls)[0][0]]\n",
        "\n",
        "            # Assign weight to corresponding samples\n",
        "            sample_weights[cls_indices] = cls_weight\n",
        "\n",
        "        # Normalize weights\n",
        "        sample_weights = sample_weights / np.sum(sample_weights)\n",
        "\n",
        "        # Create weighted sampler\n",
        "        sampler = WeightedRandomSampler(\n",
        "            weights=sample_weights,\n",
        "            num_samples=len(labels),\n",
        "            replacement=True\n",
        "        )\n",
        "\n",
        "        return sampler\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_confusion_matrix(true_labels, predicted_labels, classes):\n",
        "        \"\"\"\n",
        "        Plot and save confusion matrix\n",
        "\n",
        "        Args:\n",
        "        true_labels (array-like): True labels\n",
        "        predicted_labels (array-like): Predicted labels\n",
        "        classes (array-like): Class names\n",
        "        \"\"\"\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=classes,\n",
        "                    yticklabels=classes)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Labels')\n",
        "        plt.ylabel('True Labels')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "def prepare_data(csv_path, test_size=0.2, random_state=42, window_size=5):\n",
        "    \"\"\"\n",
        "    Prepare data with imbalanced dataset handling\n",
        "\n",
        "    Args:\n",
        "    csv_path (str): Path to the CSV file\n",
        "    test_size (float): Proportion of test set\n",
        "    random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    tuple: Prepared training and testing datasets and dataloaders\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Separate features and labels\n",
        "    feature_columns = [col for col in df.columns if col not in ['time', 'label']]\n",
        "    \n",
        "    df[feature_columns] = df[feature_columns].rolling(window=window_size, min_periods=1).mean()\n",
        "\n",
        "    X = df[feature_columns].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Analyze class distribution\n",
        "    class_dist = ImbalancedDatasetHandler.analyze_class_distribution(y)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    \n",
        "    joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = ImbalancedDatasetHandler.compute_class_weights(y_train)\n",
        "\n",
        "    # Create weighted sampler\n",
        "    train_sampler = ImbalancedDatasetHandler.create_weighted_sampler(y_train)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train = torch.FloatTensor(X_train)\n",
        "    X_test = torch.FloatTensor(X_test)\n",
        "    y_train = torch.LongTensor(y_train)\n",
        "    y_test = torch.LongTensor(y_test)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        sampler=train_sampler\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, label_encoder, class_weights\n",
        "\n",
        "def train_model(train_loader, test_loader, num_classes, class_weights, num_epochs=50, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Train the CNN model with imbalanced dataset handling\n",
        "\n",
        "    Args:\n",
        "    train_loader (DataLoader): Training data loader\n",
        "    test_loader (DataLoader): Testing data loader\n",
        "    num_classes (int): Number of output classes\n",
        "    class_weights (torch.Tensor): Weights for each class\n",
        "    num_epochs (int): Number of training epochs\n",
        "    learning_rate (float): Learning rate for optimization\n",
        "\n",
        "    Returns:\n",
        "    nn.Module: Trained model\n",
        "    \"\"\"\n",
        "    # Determine input dimension\n",
        "    input_dim = next(iter(train_loader))[0].shape[1]\n",
        "\n",
        "    # Initialize model\n",
        "    model = KeypointsCNN(input_dim, num_classes)\n",
        "\n",
        "    # Initialize loss with class weights\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_labels in test_loader:\n",
        "                outputs = model(batch_features)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                all_preds.extend(predicted.numpy())\n",
        "                all_labels.extend(batch_labels.numpy())\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "        print(f'Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def main(csv_path):\n",
        "    \"\"\"\n",
        "    Main function to run the entire pipeline with imbalanced dataset handling\n",
        "\n",
        "    Args:\n",
        "    csv_path (str): Path to the CSV file\n",
        "    \"\"\"\n",
        "    # Prepare data\n",
        "    train_loader, test_loader, label_encoder, class_weights = prepare_data(csv_path)\n",
        "\n",
        "    # Get number of classes\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "\n",
        "    # Train model\n",
        "    trained_model = train_model(\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        num_classes,\n",
        "        class_weights\n",
        "    )\n",
        "\n",
        "    # Final evaluation\n",
        "    trained_model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in test_loader:\n",
        "            outputs = trained_model(batch_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.numpy())\n",
        "            all_labels.extend(batch_labels.numpy())\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    ImbalancedDatasetHandler.plot_confusion_matrix(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        label_encoder.classes_\n",
        "    )\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        target_names=label_encoder.classes_\n",
        "    ))\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(trained_model.state_dict(), 'keypoints_cnn_model.pth')\n",
        "    print(\"\\nModel saved as 'keypoints_cnn_model.pth'\")\n",
        "\n",
        "# Reuse the previous CNN model definition from the last script\n",
        "class KeypointsCNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        \"\"\"\n",
        "        CNN Model for keypoints prediction\n",
        "\n",
        "        Args:\n",
        "        input_dim (int): Input feature dimension\n",
        "        num_classes (int): Number of output classes\n",
        "        \"\"\"\n",
        "        super(KeypointsCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(64 * (input_dim // 4), 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model\n",
        "\n",
        "        Args:\n",
        "        x (torch.Tensor): Input tensor\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: Output predictions\n",
        "        \"\"\"\n",
        "        # Reshape input to (batch_size, 1, features)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc_layers(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    # Replace with your actual CSV path\n",
        "    csv_path = r\"D:\\git_project\\workshop_lifting\\app\\Train\\Labeled_Dataset__English_.csv\"\n",
        "    main(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-00H7-H7aU8B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def evaluate_model(model, test_loader, label_encoder, num_classes):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model and display performance metrics.\n",
        "\n",
        "    Args:\n",
        "    model (nn.Module): Trained PyTorch model\n",
        "    test_loader (DataLoader): Test data loader\n",
        "    label_encoder (LabelEncoder): Encoder for label decoding\n",
        "    num_classes (int): Number of classes\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in test_loader:\n",
        "            outputs = model(batch_features)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.numpy())\n",
        "            all_labels.extend(batch_labels.numpy())\n",
        "            all_probs.extend(probs.numpy())\n",
        "\n",
        "    # Convert lists to arrays\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # 1. Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
        "\n",
        "    # 3. ROC-AUC Curve (only if more than one class)\n",
        "    if num_classes > 2:\n",
        "        y_test_bin = label_binarize(all_labels, classes=np.arange(num_classes))\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for i in range(num_classes):\n",
        "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], all_probs[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, label=f'Class {label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC-AUC Curve')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # 4. Show Some Predictions\n",
        "    print(\"\\nExample Predictions:\")\n",
        "    num_examples = min(10, len(all_labels))\n",
        "    indices = np.random.choice(len(all_labels), num_examples, replace=False)\n",
        "    for idx in indices:\n",
        "        print(f\"True: {label_encoder.classes_[all_labels[idx]]} | Predicted: {label_encoder.classes_[all_preds[idx]]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "75llBY1ydtuC",
        "outputId": "cfcc0c17-5881-4706-ef25-f834653039e9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trained_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b823df5d7a43>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
          ]
        }
      ],
      "source": [
        "evaluate_model(trained_model, test_loader, label_encoder, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "JJMte8MReAsm",
        "outputId": "925322a0-1627-497a-d00d-dd24f83f5384"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e83ce4915ff5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# à¸”à¸¶à¸‡à¸‚à¸™à¸²à¸” input à¸ˆà¸²à¸ test_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeypointsCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ],
      "source": [
        "# à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥\n",
        "input_dim = next(iter(test_loader))[0].shape[1]  # à¸”à¸¶à¸‡à¸‚à¸™à¸²à¸” input à¸ˆà¸²à¸ test_loader\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "trained_model = KeypointsCNN(input_dim, num_classes)\n",
        "trained_model.load_state_dict(torch.load('/content/keypoints_cnn_model.pth'))\n",
        "trained_model.eval()\n",
        "\n",
        "# à¸—à¸³à¸à¸²à¸£ evaluate à¹‚à¸¡à¹€à¸”à¸¥\n",
        "evaluate_model(trained_model, test_loader, label_encoder, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KeypointsCNN(\n",
              "  (conv_layers): Sequential(\n",
              "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc_layers): Sequential(\n",
              "    (0): Linear(in_features=2112, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.3, inplace=False)\n",
              "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = KeypointsCNN(input_dim=132, num_classes=5)\n",
        "model.load_state_dict(torch.load('keypoints_cnn_model.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "workshop_lifting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
