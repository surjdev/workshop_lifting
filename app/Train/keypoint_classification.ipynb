{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c88c90",
   "metadata": {},
   "source": [
    "# CNN + Machine Learning Classification for Keypoint Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72752253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42b35c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load และเตรียมข้อมูล\n",
    "df = pd.read_csv(r\"D:\\git_project\\workshop_lifting\\app\\Train\\Labeled_Dataset__English_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "647f847e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hold', 'empty'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop_unknown = df[df[\"label\"] != \"unknown\"]\n",
    "df_lift = df_drop_unknown[df_drop_unknown['label'] != \"lift\"]\n",
    "df_place = df_lift[df_lift['label'] != \"place\"]\n",
    "df_place['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "360746a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_place.drop(columns=[\"time\", \"label\"]).values\n",
    "y = df_place[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f5c6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง window ขนาด 30 frame (เช่น 1 วิ)\n",
    "window_size = 30\n",
    "stride = 15\n",
    "\n",
    "X_windows, y_windows = [], []\n",
    "\n",
    "for start in range(0, len(X) - window_size, stride):\n",
    "    end = start + window_size\n",
    "    if len(set(y[start:end])) == 1:  # เอาเฉพาะช่วงที่ label เดียวกัน\n",
    "        X_windows.append(X[start:end])\n",
    "        y_windows.append(y[start])\n",
    "\n",
    "X_windows = np.array(X_windows)  # shape: [N, 30, features]\n",
    "y_windows = np.array(y_windows)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_windows)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_windows, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = KeypointDataset(X_train, y_train)\n",
    "test_dataset = KeypointDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d879aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 3)\n",
    "        self.fc1 = nn.Linear(128 * 6, 128)  # ปรับตาม input\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # [B, features, time]\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CNNClassifier(num_features=X.shape[1], num_classes=len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a26a6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c90bdd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       empty       0.69      1.00      0.81        11\n",
      "        hold       1.00      0.69      0.81        16\n",
      "\n",
      "    accuracy                           0.81        27\n",
      "   macro avg       0.84      0.84      0.81        27\n",
      "weighted avg       0.87      0.81      0.81        27\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  0]\n",
      " [ 5 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "# รายงานผล\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Matrix ความสับสน\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78e42a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a38b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bec0e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "def pred_in_video(video_file):\n",
    "    if video_file:\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        # temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\")\n",
    "        # out_file = temp_file.name  # Get temp file path\n",
    "\n",
    "        # out = cv2.VideoWriter(\n",
    "        #                     out_file,\n",
    "        #                     cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        #                     fps,\n",
    "        #                     (frame_width, frame_height)\n",
    "        #                     )\n",
    "\n",
    "        data_dict = {'time':[]}\n",
    "        frame_number = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # timestamp = frame_number / fps\n",
    "            frame_number += 1\n",
    "\n",
    "            # Convert frame to RGB (MediaPipe requires RGB input)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process frame\n",
    "            results = pose.process(rgb_frame)\n",
    "            \n",
    "            # Draw landmarks and Record if detected\n",
    "            if results.pose_landmarks:\n",
    "                mp.solutions.drawing_utils.draw_landmarks(frame, \n",
    "                                                          results.pose_landmarks, \n",
    "                                                          mp_pose.POSE_CONNECTIONS)\n",
    "                result_landmark = results.pose_landmarks.landmark\n",
    "\n",
    "                data_dict['time'].append(frame_number / fps)\n",
    "                \n",
    "                for i in range(len(result_landmark)):\n",
    "                    column_x = f'keypoint_{i}_x'\n",
    "                    column_y = f'keypoint_{i}_y'\n",
    "                    column_z = f'keypoint_{i}_z'\n",
    "                    column_vi = f'keypoint_{i}_visibility'\n",
    "\n",
    "                    if column_x not in data_dict:\n",
    "                        data_dict[column_x] = []\n",
    "                        data_dict[column_y] = []\n",
    "                        data_dict[column_z] = []\n",
    "                        data_dict[column_vi] = []\n",
    "                    \n",
    "                    # data_dict['time'].append(frame_number / fps)\n",
    "                    data_dict[column_x].append(result_landmark[i].x)\n",
    "                    data_dict[column_y].append(result_landmark[i].y)\n",
    "                    data_dict[column_z].append(result_landmark[i].z)\n",
    "                    data_dict[column_vi].append(result_landmark[i].visibility)\n",
    "\n",
    "            # Save frame to output video\n",
    "            # out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    # out.release()\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    # converted_file = out_file.replace(\".mp4\", \"_converted.mp4\")\n",
    "    # convert_to_h264(out_file, converted_file)\n",
    "\n",
    "    # time.sleep(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a80aec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pitch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\Anaconda\\envs\\workshop_lifting\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'pitch'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# If you want to export the results:\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# df.to_csv(\"/mnt/data/dataset_test_output.csv\", index=False)\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    115\u001b[39m     velocity = compute_velocity(df.iloc[i-\u001b[32m1\u001b[39m], df.iloc[i])\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m current_pitch = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpitch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Update the state machine\u001b[39;00m\n\u001b[32m    120\u001b[39m current_state = fsm.update_state(velocity, current_pitch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\Anaconda\\envs\\workshop_lifting\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\Anaconda\\envs\\workshop_lifting\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\Anaconda\\envs\\workshop_lifting\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'pitch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Velocity Computation\n",
    "# -------------------------------------------------------------------\n",
    "def compute_velocity(prev_row, current_row):\n",
    "    \"\"\"\n",
    "    Compute the scalar velocity (magnitude) of the wrist between two consecutive rows.\n",
    "    prev_row and current_row are Pandas Series or dictionaries with keys: x, y, z, time\n",
    "    \"\"\"\n",
    "    dx = current_row['x'] - prev_row['x']\n",
    "    dy = current_row['y'] - prev_row['y']\n",
    "    dz = current_row['z'] - prev_row['z']\n",
    "    \n",
    "    dist = math.sqrt(dx**2 + dy**2 + dz**2)\n",
    "    \n",
    "    dt = current_row['time'] - prev_row['time']\n",
    "    if dt == 0:\n",
    "        # Avoid divide-by-zero; handle as velocity = 0 if times are the same\n",
    "        return 0.0\n",
    "    \n",
    "    vel = dist / dt\n",
    "    return vel\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Finite-State Machine Definition\n",
    "# -------------------------------------------------------------------\n",
    "class PickPlaceStateMachine:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vel_min=0.02,           # threshold for \"near zero\" velocity\n",
    "                 pick_angle_min=0,       # minimal pitch angle for picking\n",
    "                 pick_angle_max=20,      # maximal pitch angle for picking\n",
    "                 place_angle_min=15,     # minimal pitch angle for placing\n",
    "                 place_angle_max=30):    # maximal pitch angle for placing\n",
    "        \n",
    "        self.vel_min = vel_min\n",
    "        self.pick_angle_min = pick_angle_min\n",
    "        self.pick_angle_max = pick_angle_max\n",
    "        self.place_angle_min = place_angle_min\n",
    "        self.place_angle_max = place_angle_max\n",
    "        \n",
    "        # Possible states\n",
    "        self.state_list = [\"IDLE\", \"APPROACH\", \"GRASP\", \"TRANSPORT\", \"PLACE\"]\n",
    "        self.current_state = \"IDLE\"\n",
    "    \n",
    "    def update_state(self, velocity, pitch):\n",
    "        \"\"\"\n",
    "        Update the FSM based on velocity + pitch.\n",
    "        Return the current state.\n",
    "        \"\"\"\n",
    "        # IDLE\n",
    "        if self.current_state == \"IDLE\":\n",
    "            if (self.pick_angle_min <= pitch <= self.pick_angle_max) and (velocity > self.vel_min):\n",
    "                self.current_state = \"APPROACH\"\n",
    "\n",
    "        # APPROACH\n",
    "        elif self.current_state == \"APPROACH\":\n",
    "            if (velocity < self.vel_min) and (self.pick_angle_min <= pitch <= self.pick_angle_max):\n",
    "                self.current_state = \"GRASP\"\n",
    "            elif not (self.pick_angle_min <= pitch <= self.pick_angle_max):\n",
    "                self.current_state = \"IDLE\"\n",
    "\n",
    "        # GRASP\n",
    "        elif self.current_state == \"GRASP\":\n",
    "            if velocity > self.vel_min:\n",
    "                self.current_state = \"TRANSPORT\"\n",
    "            elif not (self.pick_angle_min <= pitch <= self.pick_angle_max):\n",
    "                self.current_state = \"IDLE\"\n",
    "\n",
    "        # TRANSPORT\n",
    "        elif self.current_state == \"TRANSPORT\":\n",
    "            if (velocity < self.vel_min) and (self.place_angle_min <= pitch <= self.place_angle_max):\n",
    "                self.current_state = \"PLACE\"\n",
    "            elif (velocity < self.vel_min):\n",
    "                self.current_state = \"IDLE\"\n",
    "\n",
    "        # PLACE\n",
    "        elif self.current_state == \"PLACE\":\n",
    "            if velocity < self.vel_min:\n",
    "                self.current_state = \"IDLE\"\n",
    "        \n",
    "        return self.current_state\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Main Routine: Reading CSV & Applying FSM\n",
    "# -------------------------------------------------------------------\n",
    "def main():\n",
    "    # STEP 1: Read CSV\n",
    "    # Replace \"time\", \"x\", \"y\", \"z\", \"pitch\" with your actual column names\n",
    "    df = pd.read_csv(\"D:\\git_project\\workshop_lifting\\dataset_test.csv\")\n",
    "    \n",
    "    df.rename(columns={\n",
    "    'Time': 'time',\n",
    "    'X': 'x',\n",
    "    'Y': 'y',\n",
    "    'Z': 'z',\n",
    "    'HandPitch': 'pitch'\n",
    "}, inplace=True)\n",
    "\n",
    "    # Ensure data is sorted by time if not already\n",
    "    df.sort_values(by='time', inplace=True, ignore_index=True)\n",
    "    \n",
    "    # STEP 2: Initialize FSM\n",
    "    fsm = PickPlaceStateMachine(\n",
    "        vel_min=0.02,           # Adjust as appropriate\n",
    "        pick_angle_min=0,       # Example range for picking\n",
    "        pick_angle_max=20,\n",
    "        place_angle_min=15,     # Example range for placing\n",
    "        place_angle_max=30\n",
    "    )\n",
    "\n",
    "    # STEP 3: Calculate velocity and update FSM for each row\n",
    "    velocities = []\n",
    "    states = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i == 0:\n",
    "            velocity = 0.0\n",
    "        else:\n",
    "            velocity = compute_velocity(df.iloc[i-1], df.iloc[i])\n",
    "        \n",
    "        current_pitch = df.iloc[i]['pitch']\n",
    "        \n",
    "        # Update the state machine\n",
    "        current_state = fsm.update_state(velocity, current_pitch)\n",
    "\n",
    "        velocities.append(velocity)\n",
    "        states.append(current_state)\n",
    "    \n",
    "    # STEP 4: Save results back to DataFrame\n",
    "    df['velocity'] = velocities\n",
    "    df['state'] = states\n",
    "    \n",
    "    # STEP 5: (Optional) Export or display results\n",
    "    # Print the first few rows\n",
    "    print(df.head(10))\n",
    "\n",
    "    # If you want to export the results:\n",
    "    # df.to_csv(\"/mnt/data/dataset_test_output.csv\", index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4351e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Mediapipe Pose setup\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# For drawing: define some colors\n",
    "GRID_COLOR = (0, 255, 0)   # Green for grid lines\n",
    "TEXT_COLOR = (0, 0, 255)   # Red for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "920faa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    \"\"\"Compute Euclidean distance between two points p1=(x1,y1) and p2=(x2,y2).\"\"\"\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e4bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cap = cv2.VideoCapture(\"\")  # or your video path\n",
    "    with mp_pose.Pose(static_image_mode=False, \n",
    "                      model_complexity=1,\n",
    "                      enable_segmentation=False,\n",
    "                      min_detection_confidence=0.5,\n",
    "                      min_tracking_confidence=0.5) as pose:\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Convert BGR to RGB for Mediapipe\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(rgb_frame)\n",
    "            \n",
    "            # If pose landmarks are detected\n",
    "            if results.pose_landmarks:\n",
    "                # Get landmark array\n",
    "                h, w, _ = frame.shape\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                \n",
    "                # Extract relevant landmarks (in normalized coords [0..1])\n",
    "                # For right foot:\n",
    "                r_heel  = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value]\n",
    "                r_index = landmarks[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value]\n",
    "                # For left foot:\n",
    "                l_heel  = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value]\n",
    "                l_index = landmarks[mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value]\n",
    "                \n",
    "                # Convert normalized coords to pixel coords\n",
    "                r_heel_xy  = (r_heel.x  * w, r_heel.y  * h)\n",
    "                r_index_xy = (r_index.x * w, r_index.y * h)\n",
    "                l_heel_xy  = (l_heel.x  * w, l_heel.y  * h)\n",
    "                l_index_xy = (l_index.x * w, l_index.y * h)\n",
    "                \n",
    "                # Decide which foot to use (the one with the largest heel-index distance).\n",
    "                # (User requested \"longest distance between feet heel and index\")\n",
    "                dist_r = euclidean_distance(r_heel_xy, r_index_xy)\n",
    "                dist_l = euclidean_distance(l_heel_xy, l_index_xy)\n",
    "                \n",
    "                if dist_r >= dist_l:\n",
    "                    foot_heel_xy  = r_heel_xy\n",
    "                    foot_index_xy = r_index_xy\n",
    "                    side_used = 'right'\n",
    "                else:\n",
    "                    foot_heel_xy  = l_heel_xy\n",
    "                    foot_index_xy = l_index_xy\n",
    "                    side_used = 'left'\n",
    "                \n",
    "                # dis_hi = distance in the X-axis only (the user’s definition suggests\n",
    "                # columns rely on x-distance). If you need pure Euclidean, adapt accordingly.\n",
    "                dis_hi = abs(foot_heel_xy[0] - foot_index_xy[0])\n",
    "                \n",
    "                # We also need y-coordinates for feet, knee, hip, shoulder\n",
    "                # For the same side or for an average? For clarity, let's use the same side as chosen above.\n",
    "                if side_used == 'right':\n",
    "                    foot_xy    = ((foot_heel_xy[0] + foot_index_xy[0]) / 2, \n",
    "                                  (foot_heel_xy[1] + foot_index_xy[1]) / 2)\n",
    "                    knee       = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value]\n",
    "                    hip        = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "                    shoulder   = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "                else:\n",
    "                    foot_xy    = ((foot_heel_xy[0] + foot_index_xy[0]) / 2, \n",
    "                                  (foot_heel_xy[1] + foot_index_xy[1]) / 2)\n",
    "                    knee       = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
    "                    hip        = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "                    shoulder   = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "                \n",
    "                knee_xy     = (knee.x * w, knee.y * h)\n",
    "                hip_xy      = (hip.x  * w, hip.y  * h)\n",
    "                shoulder_xy = (shoulder.x * w, shoulder.y * h)\n",
    "                \n",
    "                # Prepare row boundaries (from bottom to top in image coordinates):\n",
    "                y_feet     = foot_xy[1]\n",
    "                y_knee     = knee_xy[1]\n",
    "                y_hip      = hip_xy[1]\n",
    "                y_shoulder = shoulder_xy[1]\n",
    "                \n",
    "                # Because in most images, foot is at a larger y-value than the shoulder, we can define:\n",
    "                # row1: y in [y_knee, y_feet]\n",
    "                # row2: y in [y_hip,  y_knee]\n",
    "                # row3: y in [y_shoulder, y_hip]\n",
    "                # row4: y < y_shoulder\n",
    "                #\n",
    "                # If your image or coordinate system is reversed, adjust accordingly.\n",
    "\n",
    "                # Columns: define x-coordinates\n",
    "                # first_col_start  = foot_index_xy[0] + dis_hi\n",
    "                # second_col_start = first_col_start + dis_hi\n",
    "                #\n",
    "                # But your text says:\n",
    "                #   first column -> start at x value of foot index + dis_hi\n",
    "                #   second column -> start at x value of first column + dis_hi\n",
    "                #   last column -> everything beyond that\n",
    "                #\n",
    "                # We'll define them carefully. We also check if the hand is on the right or left to interpret direction.\n",
    "\n",
    "                foot_index_x = foot_index_xy[0]\n",
    "                \n",
    "                if side_used == 'right':\n",
    "                    # Typically, if the foot index is to the left of the heel for a right foot,\n",
    "                    # we define columns going to the right. So:\n",
    "                    first_col_x  = foot_index_x + dis_hi\n",
    "                    second_col_x = first_col_x + dis_hi\n",
    "                    # last column is > second_col_x\n",
    "                else:\n",
    "                    # If it's the left side, we might invert the logic (mirroring).\n",
    "                    # E.g., we define columns going to the left from foot_index_x.\n",
    "                    first_col_x  = foot_index_x - dis_hi\n",
    "                    second_col_x = first_col_x - dis_hi\n",
    "                \n",
    "                # For demonstration, let's also get the right wrist or left wrist to see where it is\n",
    "                right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "                left_wrist  = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "                \n",
    "                # We'll check which wrist to track. Suppose we track the *same side* wrist:\n",
    "                if side_used == 'right':\n",
    "                    wrist_xy = (right_wrist.x * w, right_wrist.y * h)\n",
    "                else:\n",
    "                    wrist_xy = (left_wrist.x * w, left_wrist.y * h)\n",
    "                \n",
    "                # -----------\n",
    "                # Determine which row the wrist is in:\n",
    "                # -----------\n",
    "                wx, wy = wrist_xy\n",
    "                \n",
    "                # We'll define a small helper:\n",
    "                def get_row_name(y_val):\n",
    "                    # We assume y_feet > y_knee > y_hip > y_shoulder in typical camera orientation\n",
    "                    if y_val >= y_knee:\n",
    "                        return 1  # row1\n",
    "                    elif y_hip <= y_val < y_knee:\n",
    "                        return 2  # row2\n",
    "                    elif y_shoulder <= y_val < y_hip:\n",
    "                        return 3  # row3\n",
    "                    else:\n",
    "                        return 4  # row4 (above shoulder)\n",
    "                \n",
    "                row_id = get_row_name(wy)\n",
    "                \n",
    "                # -----------\n",
    "                # Determine which column the wrist is in:\n",
    "                # -----------\n",
    "                def get_col_name(x_val):\n",
    "                    if side_used == 'right':\n",
    "                        if x_val < first_col_x:\n",
    "                            return 0  # \"before first column\" if the wrist is too close\n",
    "                        elif first_col_x <= x_val < second_col_x:\n",
    "                            return 1  # first column\n",
    "                        else:\n",
    "                            return 2  # second column or beyond\n",
    "                    else:\n",
    "                        # For the left side, we reversed the logic\n",
    "                        if x_val > first_col_x:\n",
    "                            return 0  # \"before\" in the mirrored sense\n",
    "                        elif second_col_x < x_val <= first_col_x:\n",
    "                            return 1  # first column\n",
    "                        else:\n",
    "                            return 2  # second column or beyond\n",
    "                \n",
    "                col_id = get_col_name(wx)\n",
    "                \n",
    "                # -----------\n",
    "                # Draw the grid lines\n",
    "                # -----------\n",
    "                # Draw horizontal lines (rows)\n",
    "                # row1 boundary => y_knee\n",
    "                # row2 boundary => y_hip\n",
    "                # row3 boundary => y_shoulder\n",
    "                cv2.line(frame, (0, int(y_knee)), (w, int(y_knee)), GRID_COLOR, 2)\n",
    "                cv2.line(frame, (0, int(y_hip)), (w, int(y_hip)), GRID_COLOR, 2)\n",
    "                cv2.line(frame, (0, int(y_shoulder)), (w, int(y_shoulder)), GRID_COLOR, 2)\n",
    "                \n",
    "                # Draw vertical lines (columns) based on side\n",
    "                # first_col_x, second_col_x\n",
    "                cv2.line(frame, (int(first_col_x), 0), (int(first_col_x), h), GRID_COLOR, 2)\n",
    "                cv2.line(frame, (int(second_col_x), 0), (int(second_col_x), h), GRID_COLOR, 2)\n",
    "                \n",
    "                # -----------\n",
    "                # Display the area info\n",
    "                # -----------\n",
    "                text_position = (int(wx), int(wy) - 10)\n",
    "                area_text = f\"Row={row_id}, Col={col_id}\"\n",
    "                cv2.putText(frame, area_text, text_position, \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, TEXT_COLOR, 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Optionally draw a small circle at the wrist\n",
    "                cv2.circle(frame, (int(wx), int(wy)), 8, (255, 0, 0), -1)\n",
    "            \n",
    "            cv2.imshow('Grid Demo', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c01f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Example video path\n",
    "video_path = r\"D:\\Dataset\\AI_W2_healthcare\\lifting_0215.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Draw pose landmarks for visualization\n",
    "            mp_drawing.draw_landmarks(frame,\n",
    "                                      results.pose_landmarks,\n",
    "                                      mp_pose.POSE_CONNECTIONS)\n",
    "            \n",
    "            # Get image dimensions\n",
    "            h, w, c = frame.shape\n",
    "            \n",
    "            # Helper function to get pixel coords of a specific landmark\n",
    "            def get_landmark(landmark_enum):\n",
    "                lm = results.pose_landmarks.landmark[landmark_enum]\n",
    "                return int(lm.x * w), int(lm.y * h)\n",
    "            \n",
    "            # Grab right-side landmarks (you can adapt for left side as needed)\n",
    "            r_foot_index = get_landmark(mp_pose.PoseLandmark.RIGHT_FOOT_INDEX)\n",
    "            r_heel       = get_landmark(mp_pose.PoseLandmark.RIGHT_HEEL)\n",
    "            r_knee       = get_landmark(mp_pose.PoseLandmark.RIGHT_KNEE)\n",
    "            r_hip        = get_landmark(mp_pose.PoseLandmark.RIGHT_HIP)\n",
    "            r_shoulder   = get_landmark(mp_pose.PoseLandmark.RIGHT_SHOULDER)\n",
    "            r_wrist      = get_landmark(mp_pose.PoseLandmark.RIGHT_WRIST)\n",
    "            \n",
    "            # 1) Compute the distance between foot heel and foot index (x-coords)\n",
    "            dis_hi = abs(r_foot_index[0] - r_heel[0])\n",
    "            \n",
    "            # 2) Define columns using dis_hi:\n",
    "            #    first column -> starts at x value of foot index + dis_hi\n",
    "            #    second column -> starts at x value of first column + dis_hi\n",
    "            #    last column -> everything beyond the second column\n",
    "            #\n",
    "            # For illustration, let's assume foot_index.x < heel.x, so we do:\n",
    "            # col1_x = r_foot_index[0] + dis_hi\n",
    "            # col2_x = col1_x + dis_hi\n",
    "            #\n",
    "            # If the foot index is to the right of the heel, invert the logic. \n",
    "            # A quick way is to just define a \"base\" = min(r_foot_index[0], r_heel[0]).\n",
    "            \n",
    "            foot_min_x = min(r_foot_index[0], r_heel[0])\n",
    "            col1_x = foot_min_x + dis_hi\n",
    "            col2_x = foot_min_x + 2 * dis_hi\n",
    "            \n",
    "            # 3) Define rows:\n",
    "            #    first row (Row1): from foot up to knee\n",
    "            #    second row (Row2): from knee up to hip\n",
    "            #    third row (Row3): from hip up to shoulder\n",
    "            #    last row (Row4): above the shoulder\n",
    "            \n",
    "            # Typically, the foot is the largest y-value (since y grows downward).\n",
    "            # We'll gather the relevant y-values:\n",
    "\n",
    "            foot_y     = max(r_foot_index[1], r_heel[1])  # whichever is lower on the image\n",
    "            knee_y     = r_knee[1]\n",
    "            hip_y      = r_hip[1]\n",
    "            shoulder_y = r_shoulder[1]\n",
    "            \n",
    "            # For clarity, let's sort these, though typically foot_y > knee_y > hip_y > shoulder_y\n",
    "            # But in case the detection is slightly off, or the user is in a different pose:\n",
    "            sorted_rows = sorted([foot_y, knee_y, hip_y, shoulder_y])\n",
    "            \n",
    "            # We'll rename them in ascending order (top->bottom)\n",
    "            # Actually, we want foot at the bottom, so let's do:\n",
    "            #   row1 boundary = knee_y\n",
    "            #   row2 boundary = hip_y\n",
    "            #   row3 boundary = shoulder_y\n",
    "            #\n",
    "            # If foot_y is the largest, that’s effectively the “bottom.” \n",
    "            # We can do:\n",
    "            row_knee = knee_y\n",
    "            row_hip  = hip_y\n",
    "            row_shoulder = shoulder_y\n",
    "            \n",
    "            # 4) Draw the grid lines on the image for visualization\n",
    "            #    Vertical lines\n",
    "            cv2.line(frame, (int(col1_x), 0), (int(col1_x), h), (0, 255, 0), 2)\n",
    "            cv2.line(frame, (int(col2_x), 0), (int(col2_x), h), (0, 255, 0), 2)\n",
    "            \n",
    "            #    Horizontal lines\n",
    "            cv2.line(frame, (0, int(row_knee)), (w, int(row_knee)), (0, 255, 0), 2)\n",
    "            cv2.line(frame, (0, int(row_hip)), (w, int(row_hip)), (0, 255, 0), 2)\n",
    "            cv2.line(frame, (0, int(row_shoulder)), (w, int(row_shoulder)), (0, 255, 0), 2)\n",
    "            \n",
    "            # 5) Figure out which cell the wrist is in\n",
    "            wrist_x, wrist_y = r_wrist\n",
    "            \n",
    "            #   Columns: \n",
    "            if wrist_x < col1_x:\n",
    "                col_name = \"Column 1\"\n",
    "            elif wrist_x < col2_x:\n",
    "                col_name = \"Column 2\"\n",
    "            else:\n",
    "                col_name = \"Column 3\"\n",
    "            \n",
    "            #   Rows (remember the top of the image is y=0, bottom is y=h):\n",
    "            #   Row1 (Below Knee)   : y >= row_knee\n",
    "            #   Row2 (Knee to Hip)  : row_hip <= y < row_knee\n",
    "            #   Row3 (Hip to Shoulder): row_shoulder <= y < row_hip\n",
    "            #   Row4 (Above Shoulder): y < row_shoulder\n",
    "            \n",
    "            if wrist_y >= row_knee:\n",
    "                row_name = \"Row1 (Foot-Knee)\"\n",
    "            elif row_hip <= wrist_y < row_knee:\n",
    "                row_name = \"Row2 (Knee-Hip)\"\n",
    "            elif row_shoulder <= wrist_y < row_hip:\n",
    "                row_name = \"Row3 (Hip-Shoulder)\"\n",
    "            else:\n",
    "                row_name = \"Row4 (Above Shoulder)\"\n",
    "            \n",
    "            # 6) If you want to handle “left side” vs. “right side”\n",
    "            #    (e.g., if the right wrist is actually left of the torso center),\n",
    "            #    you could do additional checks comparing wrist_x to the torso midpoint.\n",
    "            #\n",
    "            #    Example: torso_mid_x = (r_shoulder[0] + l_shoulder[0]) / 2\n",
    "            #    if wrist_x < torso_mid_x:  # the wrist is on the left side, etc.\n",
    "            #\n",
    "            #    For simplicity, we skip that in this snippet.\n",
    "\n",
    "            # 7) Overlay text on the image showing the wrist's cell\n",
    "            label_text = f\"Wrist in {col_name}, {row_name}\"\n",
    "            cv2.putText(frame, label_text, (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show the result using OpenCV window (or switch to matplotlib if desired)\n",
    "        cv2.imshow(\"Pose Grid\", cv2.resize(frame,(800,800)))\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # Press ESC to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b1ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Example video path\n",
    "video_path = r\"D:\\Dataset\\AI_W2_healthcare\\lifting_0215.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR to RGB for processing\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Draw pose landmarks for visualization\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "\n",
    "            # Get image dimensions\n",
    "            h, w, c = frame.shape\n",
    "\n",
    "            # Helper: convert landmark to pixel coordinates\n",
    "            def get_landmark(landmark_enum):\n",
    "                lm = results.pose_landmarks.landmark[landmark_enum]\n",
    "                return int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "            # Grab right-side landmarks (adapt if needed for left side)\n",
    "            r_foot_index = get_landmark(mp_pose.PoseLandmark.RIGHT_FOOT_INDEX)\n",
    "            r_heel       = get_landmark(mp_pose.PoseLandmark.RIGHT_HEEL)\n",
    "            r_knee       = get_landmark(mp_pose.PoseLandmark.RIGHT_KNEE)\n",
    "            r_hip        = get_landmark(mp_pose.PoseLandmark.RIGHT_HIP)\n",
    "            r_shoulder   = get_landmark(mp_pose.PoseLandmark.RIGHT_SHOULDER)\n",
    "            r_wrist      = get_landmark(mp_pose.PoseLandmark.RIGHT_WRIST)\n",
    "\n",
    "            # 1) Compute horizontal distance between foot heel and foot index\n",
    "            dis_hi = abs(r_foot_index[0] - r_heel[0])\n",
    "\n",
    "            # Use the foot index x-coordinate as a reference boundary\n",
    "            foot_index_x = r_foot_index[0]\n",
    "            wrist_x, wrist_y = r_wrist\n",
    "\n",
    "            # 2) Define horizontal grid lines (rows)\n",
    "            # Get y-coordinates of foot, knee, hip, shoulder.\n",
    "            # Note: In images, y increases downward.\n",
    "            foot_y     = max(r_foot_index[1], r_heel[1])  # lower (larger y) of the two\n",
    "            knee_y     = r_knee[1]\n",
    "            hip_y      = r_hip[1]\n",
    "            shoulder_y = r_shoulder[1]\n",
    "            \n",
    "            # Sort these y-values (ascending order: top->bottom)\n",
    "            row_lines = sorted([shoulder_y, hip_y, knee_y, foot_y])\n",
    "            # Draw horizontal lines across the image\n",
    "            for y_line in row_lines:\n",
    "                cv2.line(frame, (0, y_line), (w, y_line), (0, 0, 255), 2)\n",
    "\n",
    "            # 3) Define vertical grid lines (columns) based on wrist location\n",
    "            if wrist_x >= foot_index_x:\n",
    "                # If wrist is on the right side of the foot index,\n",
    "                # use boundaries: foot_index_x, col1_end, col2_end.\n",
    "                col1_end = foot_index_x + dis_hi\n",
    "                col2_end = foot_index_x + 2 * dis_hi\n",
    "                col_lines = sorted([foot_index_x, int(col1_end), int(col2_end)])\n",
    "            else:\n",
    "                # If wrist is on the left side,\n",
    "                # define: col1_start (to the left of foot_index_x) and col2_end (further left).\n",
    "                col1_start = foot_index_x - dis_hi\n",
    "                col2_end = foot_index_x - 2 * dis_hi\n",
    "                col_lines = sorted([foot_index_x, int(col1_start), int(col2_end)])\n",
    "\n",
    "            # Draw vertical lines from top to bottom\n",
    "            for x_line in col_lines:\n",
    "                cv2.line(frame, (x_line, 0), (x_line, h), (0, 0, 255), 2)\n",
    "\n",
    "            # Optionally, you can now compute which grid cell the wrist is in.\n",
    "            # (This part is left as an exercise for further customization.)\n",
    "\n",
    "            # Overlay text showing wrist coordinates\n",
    "            label_text = f\"Wrist: ({wrist_x}, {wrist_y})\"\n",
    "            cv2.putText(frame, label_text, (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Resize and display the result\n",
    "        display_frame = cv2.resize(frame, (800, 800))\n",
    "        cv2.imshow(\"Pose Grid\", display_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # Press ESC to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "526a825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Video path\n",
    "video_path = r\"D:\\Dataset\\AI_W2_healthcare\\lifting_0215.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                  min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR to RGB for processing\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Draw the pose landmarks for visualization\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Get image dimensions\n",
    "            h, w, c = frame.shape\n",
    "\n",
    "            # Helper function to convert a landmark to pixel coordinates\n",
    "            def get_landmark(landmark_enum):\n",
    "                lm = results.pose_landmarks.landmark[landmark_enum]\n",
    "                return int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "            # Get right-side landmarks (adapt if needed for left side)\n",
    "            r_foot_index = get_landmark(mp_pose.PoseLandmark.RIGHT_FOOT_INDEX)\n",
    "            r_heel       = get_landmark(mp_pose.PoseLandmark.RIGHT_HEEL)\n",
    "            r_knee       = get_landmark(mp_pose.PoseLandmark.RIGHT_KNEE)\n",
    "            r_hip        = get_landmark(mp_pose.PoseLandmark.RIGHT_HIP)\n",
    "            r_shoulder   = get_landmark(mp_pose.PoseLandmark.RIGHT_SHOULDER)\n",
    "            r_wrist      = get_landmark(mp_pose.PoseLandmark.RIGHT_WRIST)\n",
    "\n",
    "            # 1) Compute the horizontal distance between the foot heel and foot index\n",
    "            dis_hi = abs(r_foot_index[0] - r_heel[0])\n",
    "            # Use the foot index's x-coordinate as a reference boundary\n",
    "            foot_index_x = r_foot_index[0]\n",
    "            wrist_x, wrist_y = r_wrist\n",
    "\n",
    "            # 2) Define row boundaries using landmark y-coordinates\n",
    "            # For a standing person, y increases downward.\n",
    "            # Collect y-coordinates for shoulder, hip, knee, and foot.\n",
    "            foot_y     = max(r_foot_index[1], r_heel[1])  # Lower of the two (largest y)\n",
    "            knee_y     = r_knee[1]\n",
    "            hip_y      = r_hip[1]\n",
    "            shoulder_y = r_shoulder[1]\n",
    "            # Sort from top (smallest y) to bottom (largest y)\n",
    "            row_lines = sorted([shoulder_y, hip_y, knee_y, foot_y])\n",
    "            \n",
    "            # Draw horizontal grid lines (across the full width)\n",
    "            for y_line in row_lines:\n",
    "                cv2.line(frame, (0, y_line), (w, y_line), (0, 0, 255), 2)\n",
    "\n",
    "            # Determine in which row the wrist is located:\n",
    "            # sorted order: row_lines[0] (shoulder), row_lines[1] (hip), row_lines[2] (knee), row_lines[3] (foot)\n",
    "            if wrist_y >= row_lines[2]:\n",
    "                row_num = 1  # between knee and foot\n",
    "            elif wrist_y >= row_lines[1]:\n",
    "                row_num = 2  # between hip and knee\n",
    "            elif wrist_y >= row_lines[0]:\n",
    "                row_num = 3  # between shoulder and hip\n",
    "            else:\n",
    "                row_num = 4  # above shoulder\n",
    "\n",
    "            # 3) Define vertical grid boundaries (columns) based on wrist location relative to foot index\n",
    "            if wrist_x >= foot_index_x:\n",
    "                # Wrist is on the right side:\n",
    "                col1_end = foot_index_x + dis_hi\n",
    "                col2_end = foot_index_x + 2 * dis_hi\n",
    "                col_lines = sorted([foot_index_x, int(col1_end), int(col2_end)])\n",
    "                # Determine column for right-side wrist:\n",
    "                if wrist_x < col1_end:\n",
    "                    col_num = 1\n",
    "                elif wrist_x < col2_end:\n",
    "                    col_num = 2\n",
    "                else:\n",
    "                    col_num = 3\n",
    "            else:\n",
    "                # Wrist is on the left side:\n",
    "                col1_start = foot_index_x - dis_hi\n",
    "                col2_end = foot_index_x - 2 * dis_hi\n",
    "                col_lines = sorted([foot_index_x, int(col1_start), int(col2_end)])\n",
    "                # Determine column for left-side wrist:\n",
    "                if wrist_x >= col1_start:\n",
    "                    col_num = 1\n",
    "                elif wrist_x >= col2_end:\n",
    "                    col_num = 2\n",
    "                else:\n",
    "                    col_num = 3\n",
    "\n",
    "            # Draw vertical grid lines (from top to bottom)\n",
    "            for x_line in col_lines:\n",
    "                cv2.line(frame, (x_line, 0), (x_line, h), (0, 0, 255), 2)\n",
    "\n",
    "            # 4) Overlay the grid cell information on the image\n",
    "            label_text = f\"Wrist in Row {row_num}, Column {col_num}\"\n",
    "            cv2.putText(frame, label_text, (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Resize and display the resulting frame\n",
    "        display_frame = frame\n",
    "        cv2.imshow(\"Pose Grid\", cv2.resize(display_frame,(int(w*0.5),int(h*0.5))))\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # Press ESC to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb6d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_lifting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
